{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by importing packages and setting the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get occurrence data for set of species\n",
    "from shapely.affinity import scale\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import transform\n",
    "from shapely.geometry import Polygon\n",
    "import pandas as pd\n",
    "from pygbif.species import name_backbone\n",
    "from pygbif import occurrences as occ\n",
    "import geopandas as gpd\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "file_dir=r'C:/Users/Mark.Rademaker/PycharmProjects/InternshipNaturalis/venv/github_trait_geo_diverse_dl/trait-geo-diverse-dl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open shapefile\n",
    "# Set filepath (fix path relative to yours)\n",
    "dist = file_dir+\"/data/IUCN_mammal_ranges/TERRESTRIAL_MAMMALS.shp\"\n",
    "\n",
    "# Read file using gpd.read_file()\n",
    "dist_shp = gpd.read_file(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get backbone taxonomy code for each species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dataframes to be concatenated and filtered\n",
    "occ_all_species = pd.read_csv(file_dir+\"/data/SQL_raw_gbif/occurrences_all_species.csv\")\n",
    "df = occ_all_species[occ_all_species['label'].str.contains(\" \")]\n",
    "\n",
    "print(\"oc_all_species n.rows: \",len(occ_all_species.index))\n",
    "print(\"df n.rows: \",len(occ_all_species.index))\n",
    "\n",
    "#Get unique label names\n",
    "labels=df[\"label\"].tolist()\n",
    "unique_labels=df[\"label\"].unique()\n",
    "\n",
    "names = []\n",
    "back_key =[]\n",
    "remaining_labels=[]\n",
    "\n",
    "#Get backbone associated species names and taxon keys\n",
    "for item in unique_labels:\n",
    "    if \"species\" in name_backbone(item):\n",
    "        i = name_backbone(item)['species']\n",
    "        j = name_backbone(item)['usageKey']\n",
    "        #print(item,j,i)\n",
    "        names.append(i)\n",
    "        back_key.append(j)\n",
    "    else:\n",
    "        remaining_labels.append(item)\n",
    "print(len(names))\n",
    "for item in remaining_labels:\n",
    "    value=name_backbone(item)['usageKey']\n",
    "    back_key.append(value)\n",
    "    names.append(item)\n",
    "print(len(names))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put into DataFrame\n",
    "df=pd.DataFrame({\"label\": unique_labels,\"back_key\": back_key,\"species\": names},columns=[\"label\",\"back_key\",\"species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate with occurrence data, dataframe, drop na's \n",
    "df2=pd.merge(occ_all_species,df,how=\"left\",on=\"label\")\n",
    "print(\"df2 n.rows:\", len(df2.index))\n",
    "\n",
    "df2 = df2[pd.notnull(df2['species'])]\n",
    "df2 = df2[pd.notnull(df2['decimal_latitude'])]\n",
    "df2 = df2[pd.notnull(df2['decimal_longitude'])]\n",
    "print(\"df2 without na's n.rows:\", len(df2.index))\n",
    "\n",
    "df2[\"back_key\"]=df2[\"back_key\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of species\n",
    "species = df2[\"species\"].unique()\n",
    "species.sort()\n",
    "\n",
    "#save separate dataframe for each species as csv file \n",
    "for spec in species:\n",
    "    data=df2.loc[df2['species'] == spec]\n",
    "    if len(data.index)>= 10:\n",
    "        spec=spec.replace(\" \",\"_\")\n",
    "        print(\"%s\"%spec, len(data.index))\n",
    "        data.to_csv(file_dir+'/data/SQL_raw_gbif/%s_raw_data.csv'%spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter data based on lon-lat decimals (>2), unique lon-lat locations, IUCN species range and year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create txt file with name of species included after filtering\n",
    "taxa_list=open(file_dir+'/data/SQL_filtered_gbif/taxa_list.txt',\"w\")\n",
    "\n",
    "for spec in species:\n",
    "    data=df2.loc[df2['species'] == spec]\n",
    "    \n",
    "    if len(data.index)>= 10:\n",
    "        spec = spec.replace(\" \",\"_\")\n",
    "        print(\"processing species %s\"%spec)\n",
    "        #check capriolus data as example\n",
    "        data=pd.read_csv(file_dir+'/data/SQL_raw_gbif/%s_raw_data.csv'%spec)\n",
    "        print(\"length data\", len(data.index))\n",
    "\n",
    "        #check number of decimals longitude and latitude\n",
    "        str_lat=(pd.Series.tolist(data[\"decimal_latitude\"].astype(str)))\n",
    "        str_lon=(pd.Series.tolist(data[\"decimal_longitude\"].astype(str)))\n",
    "        dec_lat=[]\n",
    "        dec_lon=[]\n",
    "\n",
    "        for i in range(len(str_lat)):\n",
    "       # print(\"row %s\"%i)\n",
    "            if \"e\" in str_lat[i]:\n",
    "                str_lat[i]=\"0.00\"\n",
    "                decla = str_lat[i].split(\".\")[1]\n",
    "                print(i, str_lat[i],decla)\n",
    "                dec_lat.append(int(len(decla)))\n",
    "            else:\n",
    "                decla = str_lat[i].split(\".\")[1]\n",
    "                #print(str_lat[i],decla)\n",
    "                dec_lat.append(int(len(decla)))\n",
    "        for i in range(len(str_lon)):\n",
    "            declo=str_lon[i].split(\".\")[1]\n",
    "            dec_lon.append(int(len(declo)))\n",
    "        #x.split(\".\")[1] for x in str_lat]\n",
    "\n",
    "\n",
    "        #dec_lon=[x.split(\".\")[1] for x in str_lon]\n",
    "        #dec_lat=[int(len(x)) for x in dec_lat]\n",
    "        #dec_lon=[int(len(x)) for x in dec_lon]\n",
    "        data[\"dec_lat\"]=dec_lat\n",
    "        data[\"dec_lon\"]=dec_lon\n",
    "\n",
    "        #filter only include those with min. 2 points\n",
    "        data=data[data[\"dec_lat\"] >= 2]\n",
    "        data=data[data[\"dec_lon\"] >= 2]\n",
    "        print(\"length only including lon-lat 2 decimals\",len(data.index))\n",
    "\n",
    "        ##turn lat/lon into  set of points\n",
    "        #data['decimal_latitude']=data['decimal_latitude'].round(2)\n",
    "        #data['decimal_longitude']=data['decimal_longitude'].round(2)\n",
    "        data['coordinates'] = list(zip(data[\"decimal_longitude\"], data[\"decimal_latitude\"]))\n",
    "        data['lonlat'] = list(zip(data[\"decimal_longitude\"], data[\"decimal_latitude\"]))\n",
    "        data['coordinates'] = data[\"coordinates\"].apply(Point)\n",
    "\n",
    "        #only keep records with unique lon-lat \n",
    "        data = data.drop_duplicates('lonlat')\n",
    "        print(\"length unique lon-lat\",len(data.index))\n",
    "\n",
    "\n",
    "        ###########################################\n",
    "        ###########################################\n",
    "        ##Access the relevant shapefiles for the species\n",
    "        speci=spec.replace(\"_\",\" \")\n",
    "        dist_shp_spec = dist_shp[dist_shp[\"binomial\"]== \"%s\"%speci]\n",
    "          poly_spec = dist_shp_spec[[\"geometry\"]]\n",
    "      ##merge the polygons\n",
    "        iucn_poly_spec= poly_spec.unary_union\n",
    "        Q3 = iucn_poly_spec.simplify(0.3)\n",
    "        Q3\n",
    "\n",
    "        if Q3.is_valid== False:\n",
    "            Q3 = Q3.buffer(0)\n",
    "\n",
    "        condition_list=[]\n",
    "\n",
    "        for point in data[\"coordinates\"]:\n",
    "            output= point.within(Q3)\n",
    "            condition_list.append(output)\n",
    "\n",
    "        #keep records that are in species range\n",
    "        data[\"in_dist_polygon\"]=condition_list\n",
    "        data2=data[data.in_dist_polygon == True]\n",
    "        print(\"length in species dist polygon\",len(data2.index))\n",
    "\n",
    "        ########################################################\n",
    "        #########################################################set date column to datetime format and extract year\n",
    "        data2['event_date'] = pd.to_datetime(data2['event_date'])\n",
    "        data2['year'] = data2['event_date'].dt.year\n",
    "        data2['month']= data2['event_date'].dt.month\n",
    "\n",
    "        #set date column to datetime format and extract year\n",
    "        data2['event_date'] = pd.to_datetime(data2['event_date'])\n",
    "        data2['year'] = data2['event_date'].dt.year\n",
    "        data2['month']= data2['event_date'].dt.month\n",
    "\n",
    "        #only include observations >1900\n",
    "        data3=data2[data2.year >= 1900]\n",
    "        print(\"length observationas >1900\", len(data3.index))\n",
    "        #save to csv\n",
    "        if len(data3.index)>=10:\n",
    "            data3.to_csv(file_dir+'/data/SQL_filtered_gbif/%s_filtered_data.csv'%spec)\n",
    "            taxa_list.write(spec+\"\\n\")\n",
    "            \n",
    "#close text file\n",
    "taxa_list.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
