{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import plotting_extent\n",
    "from natsort import natsorted\n",
    "import gdal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas\n",
    "import rasterio\n",
    "import pycrs\n",
    "import math\n",
    "\n",
    "file_dir=r'C:/Users/Mark.Rademaker/PycharmProjects/InternshipNaturalis/trait-geo-diverse-dl/data_GIS_extended'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access file with list of taxa names\n",
    "taxa=pd.read_csv(file_dir+\"/data/SQL_filtered_gbif/taxa_list.txt\",header=None)\n",
    "taxa.columns=[\"taxon\"]\n",
    "\n",
    "species_occ_dict={}\n",
    "\n",
    "for i in taxa[\"taxon\"]:\n",
    "    taxon_data = pd.read_csv(file_dir+\"/data/SQL_filtered_gbif/%s_filtered_data.csv\"%i)\n",
    "    #add species dataframe to dict\n",
    "    species_occ_dict[\"%s\"%i] = taxon_data  \n",
    "    #check whether all species have been included and inspect dictionary\n",
    "if len(species_occ_dict.keys())==len(taxa[\"taxon\"]):\n",
    "    print(\"All species dataframes now in dictionary\")\n",
    "else:\n",
    "    print(\"Error: not all species dataframe included\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "- First read in occurrence data\n",
    "- Create a copy that we can use in the original state later\n",
    "- Create a buffer around each occurrence point, merge it into a single polygon\n",
    "- Clip the environmental raster based on this extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in species_occ_dict:  \n",
    "    #load occurrence data and set initial projection\n",
    "    data=species_occ_dict[key]\n",
    "    #print(data.columns)\n",
    "    spec = key\n",
    "    print(spec)\n",
    "\n",
    "    data['coordinates'] = list(zip(data[\"decimal_longitude\"], data[\"decimal_latitude\"]))\n",
    "    data['coordinates'] = data[\"coordinates\"].apply(Point)\n",
    "    data[\"present/pseudo_absent\"]=1\n",
    "    geo_data=geopandas.GeoDataFrame(data, geometry='coordinates',crs={'init' :'epsg:4326'})\n",
    "\n",
    "    #change projection to azimuthal equidistant to calculate 1000km buffer around point\n",
    "    geo_data = geo_data.to_crs({'init': 'esri:54032'}) \n",
    "    buffer=geo_data.buffer(1000*1000)\n",
    "    buffer=buffer.to_crs(epsg=4326)\n",
    "\n",
    "    #create single large polygon from individual buffers\n",
    "    union_buffer=buffer.unary_union\n",
    "\n",
    "    #first clip the raster based on this extend \n",
    "    raster=rasterio.open(file_dir+'/data/GIS/env_stacked/stacked_env_variables.tif')\n",
    "    #specify output tif:\n",
    "    out_tif = file_dir+'/data/GIS/spec_stacked_raster_clip/%s_raster_clip.tif'%spec\n",
    "\n",
    "    #clip the raster:\n",
    "    out_img, out_transform = mask(dataset=raster, shapes=[union_buffer],crop=True)\n",
    "\n",
    "    # Copy the metadata\n",
    "    out_meta = raster.meta.copy()\n",
    "\n",
    "    # Parse EPSG code\n",
    "    epsg_code = int(raster.crs.data['init'][5:])\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_img.shape[1],\n",
    "                     \"width\": out_img.shape[2],\n",
    "                     \"transform\": out_transform,\n",
    "                     \"crs\": pycrs.parse.from_epsg_code(epsg_code).to_proj4()})\n",
    "\n",
    "    with rasterio.open(out_tif, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect whether clip was correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the first band of the clipped raster for all species\n",
    "for key in species_occ_dict:\n",
    "    ##### Extract occurrence point to plot on the raster (see if correct area was clipped)\n",
    "    data=species_occ_dict[key]\n",
    "    spec = key\n",
    "    data['coordinates'] = list(zip(data[\"decimal_longitude\"], data[\"decimal_latitude\"]))\n",
    "    data['coordinates'] = data[\"coordinates\"].apply(Point)\n",
    "    geo_data=geopandas.GeoDataFrame(data, geometry='coordinates',crs={'init' :'epsg:4326'})\n",
    "    ####open the clipped raster\n",
    "    clipped = rasterio.open(file_dir+'/data/GIS/spec_stacked_raster_clip/%s_raster_clip.tif'%spec)\n",
    "    array = clipped.read(1)\n",
    "    array_data = clipped.read(1,masked=True)\n",
    "    array_meta = clipped.profile\n",
    "   \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(array_data,cmap=\"gist_earth\",interpolation=\"none\",vmin=0,\n",
    "    # Here you must set the spatial extent or else the data will not line up with your geopandas layer\n",
    "    extent=plotting_extent(clipped),)\n",
    "    spec_plots_points=geo_data[\"coordinates\"]\n",
    "    spec_plots_points.plot(ax=ax,\n",
    "                       marker='o',\n",
    "                       markersize=20,\n",
    "                       color='red')\n",
    "    ax.set_title(\"%s \\n Raster clip and occurrence points\"%spec,\n",
    "             fontsize=20)\n",
    "    plt.show()\n",
    "#Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "- now that we have the clipped raster we can use it to try and make a random selection of pseudo absence points\n",
    "- we first open the raster\n",
    "- then we separate those cells that actually contain pixel values (excluding the sea)\n",
    "- we calculate the longitude and latitude of the centre point of these cells <br>\n",
    "  (the environmental variable values do not vary within each cell so it doesn't matter if each points is in the centre)\n",
    "- we make a random selection of 10.0000 positions\n",
    "- we add the longitude and latitude values of these to to the dataset and export it  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_path=file_dir+'/data/GIS/env_stacked/stacked_env_variables.tif'\n",
    "r2=gdal.Open(stack_path)\n",
    "\n",
    "\n",
    "for key in species_occ_dict: \n",
    "    print(key)\n",
    "    #lon_lat presence points\n",
    "    presence_data = species_occ_dict[key]\n",
    "    presence_data[\"present/pseudo_absent\"]=1\n",
    "    spec = key\n",
    "    long=presence_data[\"decimal_longitude\"]\n",
    "    lati=presence_data[\"decimal_latitude\"]\n",
    "    long=pd.Series.tolist(long)\n",
    "    lati=pd.Series.tolist(lati)\n",
    "   \n",
    "    \n",
    "    ###############################################################################################################################\n",
    "    \n",
    "    ##############\n",
    "    #sample random locations from specie raster clip excluding sea and presence cells\n",
    "    ##############\n",
    "    \n",
    "    #read raster\n",
    "    #src=rasterio.open(file_dir+'/data/GIS/spec_stacked_raster_clip/%s_raster_clip.tif'%spec)\n",
    "    #array=src.read_masks(1)\n",
    "    \n",
    "    #set raster cell mask values of presence locations to 1\n",
    "    #for i in range(0,len(presence_data)):\n",
    "     #   row,col=src.index(long[i],lati[i])\n",
    "      #  array[row,col]=1\n",
    "    \n",
    "    #fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    #ax.imshow(array,cmap=\"gray\")\n",
    "    #ax.set_title(\"%s\"%spec,\n",
    "     #        fontsize=20)\n",
    "   # plt.show()\n",
    "   # print(len(presence_data), \"number of presences\")\n",
    "    \n",
    "    #(y_index, x_index) = np.nonzero(array > 1) #ensures sampling from array with values >1 (excluding presences and no-data/sea)\n",
    "    \n",
    "    \n",
    "   # r = gdal.Open(file_dir+'/data/GIS/spec_stacked_raster_clip/%s_raster_clip.tif'%spec)\n",
    "   # (upper_left_x, x_size, x_rotation, upper_left_y, y_rotation, y_size) = r.GetGeoTransform()\n",
    "   # x_coords = x_index * x_size + upper_left_x + (x_size / 2) #add half the cell size\n",
    "   # y_coords = y_index * y_size + upper_left_y + (y_size / 2) #to centre the point\n",
    "\n",
    "    #lon_lat_array=np.stack((x_coords,y_coords)).T\n",
    "\n",
    "    #random_sample_size=0\n",
    "    #len_p=int(len(presence_data))\n",
    "    \n",
    "    #if len_p >1000:\n",
    "     #   random_sample_size=len_p\n",
    "    #else: \n",
    "     #   random_sample_size=1000\n",
    "   \n",
    "    #inner_random_sample_lon_lats=lon_lat_array[np.random.choice(lon_lat_array.shape[0], random_sample_size, replace=False), :] ##\n",
    "    #print(len(inner_random_sample_lon_lats), \"number of inner pseudo absences\")\n",
    "\n",
    "    ############################################################################\n",
    "    #sample random locations from world raster excluding sea and presence cells\n",
    "    ############################################################################\n",
    "    \n",
    "    src=rasterio.open(stack_path)\n",
    "    array=src.read_masks(1)\n",
    "    \n",
    "    #set raster cell mask values of presence locations to 1\n",
    "    for i in range(0,len(presence_data)):\n",
    "        row,col=src.index(long[i],lati[i])\n",
    "        array[row,col]=1\n",
    "    \n",
    "    #fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    #ax.imshow(array,cmap=\"gray\")\n",
    "    #ax.set_title(\"%s\"%spec,\n",
    "     #        fontsize=20)\n",
    "   # plt.show()\n",
    "    #print(len(presence_data), \"number of presences\")\n",
    "    \n",
    "    (y_index_2, x_index_2) = np.nonzero(array > 1) #ensures sampling from array with values >1 (excluding presences and no-data/sea)\n",
    "    \n",
    "    r=r2\n",
    "    (upper_left_x, x_size, x_rotation, upper_left_y, y_rotation, y_size) = r.GetGeoTransform()\n",
    "    x_coords = x_index_2 * x_size + upper_left_x + (x_size / 2) #add half the cell size\n",
    "    y_coords = y_index_2 * y_size + upper_left_y + (y_size / 2) #to centre the point\n",
    "\n",
    "    lon_lat_array=np.stack((x_coords,y_coords)).T\n",
    "\n",
    "    random_sample_size=0\n",
    "    len_p=int(len(presence_data))\n",
    "    \n",
    "    if len_p >1000:\n",
    "        random_sample_size=len_p\n",
    "    else: \n",
    "        random_sample_size=2000\n",
    "   \n",
    "    outer_random_sample_lon_lats=lon_lat_array[np.random.choice(lon_lat_array.shape[0], random_sample_size, replace=False), :] ##\n",
    "    #print(len(outer_random_sample_lon_lats), \"number of outer pseudo absences\")\n",
    "    \n",
    "     \n",
    "    ###############################################################################################################################\n",
    "    \n",
    "    #Add random points to dataset\n",
    "    lon=[]\n",
    "    lat=[]\n",
    "    psa=[0]*(random_sample_size)\n",
    "    taxon=[\"%s\"%spec]*(random_sample_size)\n",
    "    gbif=[\"no_id\"]*(random_sample_size)\n",
    "\n",
    "   # for item in inner_random_sample_lon_lats:\n",
    "    #    longitude=item[0]\n",
    "    #    latitude=item[1]\n",
    "    #    lon.append(longitude)\n",
    "    #    lat.append(latitude)\n",
    "\n",
    "    for item in outer_random_sample_lon_lats:\n",
    "        longitude=item[0]\n",
    "        latitude=item[1]\n",
    "        lon.append(longitude)\n",
    "        lat.append(latitude)\n",
    "        \n",
    "    ###Dataset including n pseudo-absence points for capriolus capriolus\n",
    "    new_data=pd.DataFrame({\"gbif_id\": gbif,\"taxon_name\":taxon,\"decimal_longitude\": lon, \"decimal_latitude\":lat, \"present/pseudo_absent\": psa})\n",
    "    data=pd.concat([presence_data,new_data],ignore_index=True)\n",
    "    data=data[['taxon_name','gbif_id','decimal_longitude','decimal_latitude','present/pseudo_absent']]\n",
    "    data[\"taxon_name\"]=spec\n",
    "    data[\"row_n\"]=np.arange(len(data))\n",
    "     \n",
    "    long=data[\"decimal_longitude\"]\n",
    "    lati=data[\"decimal_latitude\"]\n",
    "    long=pd.Series.tolist(long)\n",
    "    lati=pd.Series.tolist(lati)\n",
    "    \n",
    "    #print(len(data),\"lenght data with pseudo absences pre-filtering\")\n",
    "    \n",
    "    #read raster\n",
    "    src=rasterio.open(stack_path)\n",
    "    array=src.read_masks(1)\n",
    "    \n",
    "    ##remove potential presence locations in the sea##\n",
    "    #for i in range(1,182):\n",
    "     #   print(i)\n",
    "      #  array=src.read_masks(i)\n",
    "       # for j in range(0,len(data)):\n",
    "        #    row,col=src.index(long[j],lati[j])\n",
    "         #   if array[row,col] == 0:\n",
    "          #      data=data[data.row_n != j]     \n",
    "    #print(len(data), \"length data with pseudo absences post-filtering\")\n",
    "    \n",
    "    \n",
    "    data=data.reset_index(drop=True)\n",
    "    data.to_csv(file_dir + \"/data/spec_ppa/%s_ppa_dataframe.csv\"%spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 \n",
    "- finally we can extract the environmental variable values underneath the occurrence and pseudo-absence points\n",
    "- we need to scale these environmental values for later training by taking their mean and std_dev\n",
    "- below is a code snippet, but because it requires a long time to run in jupyter the process is best split (see extract_env_variables1-4.py files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access file with list of taxa names\n",
    "taxa=pd.read_csv(file_dir+\"/data/SQL_filtered_gbif/taxa_list.txt\",header=None)\n",
    "taxa.columns=[\"taxon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster=rasterio.open(file_dir+'/data/GIS/env_stacked/stacked_env_variables.tif')\n",
    "\n",
    "\n",
    "array = raster.read()\n",
    "profile=raster.profile\n",
    "\n",
    "with open(file_dir+'/data/GIS/env_bio_mean_std.txt','w+') as file:\n",
    "    file.write(\"band\"+\"\\t\"+\"mean\"+\"\\t\"+\"std_dev\"+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "\n",
    "    \n",
    "for i in range(1,186):\n",
    "    print(i)\n",
    "    profile.update(count=1)\n",
    "    band=raster.read(i)\n",
    "    band[band < -9999] = -9999\n",
    "    where_are_NaNs = np.isnan(band)\n",
    "    band[where_are_NaNs] = -9999\n",
    "    band_masked = np.ma.masked_array(band, mask=(band == -9999))\n",
    "\n",
    "    mean=band_masked.mean()\n",
    "    std_dev=np.std(band_masked)\n",
    "\n",
    "    #mean=np.nanmean(band)\n",
    "    print(mean)\n",
    "    #std_dev=np.nanstd(band)\n",
    "    print(std_dev)\n",
    "\n",
    "    with open(file_dir+'/data/GIS/env_bio_mean_std.txt','a') as file:\n",
    "        file.write(str(i)+\"\\t\"+str(mean)+\"\\t\"+str(std_dev)+\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in species_occ_dict:    \n",
    "    #load occurrence data and set initial projection\n",
    "    data=species_occ_dict[key]\n",
    "    print(\"processing \"+key)\n",
    "    spec = key\n",
    "\n",
    "\n",
    "    data['coordinates'] = list(zip(data[\"decimal_longitude\"], data[\"decimal_latitude\"]))\n",
    "    data['coordinates'] = data[\"coordinates\"].apply(Point)\n",
    "    data[\"present/pseudo_absent\"]=1\n",
    "    geo_data=geopandas.GeoDataFrame(data, geometry='coordinates',crs={'init' :'epsg:4326'})\n",
    "\n",
    "    #change projection to azimuthal equidistant to calculate 1000km buffer around point\n",
    "    geo_data = geo_data.to_crs({'init': 'esri:54032'}) \n",
    "    buffer=geo_data.buffer(1000*1000)\n",
    "    buffer=buffer.to_crs(epsg=4326)\n",
    "\n",
    "    #create single large polygon from individual buffers\n",
    "    union_buffer=buffer.unary_union\n",
    "\n",
    "    #first clip the raster based on this extend \n",
    "    raster=rasterio.open(file_dir+'/data/GIS/env_stacked/stacked_env_variables.tif')\n",
    "    #specify output tif:\n",
    "    out_tif = file_dir+'/data/GIS/spec_stacked_raster_clip/%s_raster_clip.tif'%spec\n",
    "\n",
    "    #clip the raster:\n",
    "    out_img, out_transform = mask(dataset=raster, shapes=[union_buffer],crop=True)\n",
    "   \n",
    "    # Copy the metadata\n",
    "    out_meta = raster.meta.copy()\n",
    "\n",
    "    # Parse EPSG code\n",
    "    epsg_code = int(raster.crs.data['init'][5:])\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_img.shape[1],\n",
    "                     \"width\": out_img.shape[2],\n",
    "                     \"transform\": out_transform,\n",
    "                     \"crs\": pycrs.parse.from_epsg_code(epsg_code).to_proj4()})\n",
    "\n",
    "    with rasterio.open(out_tif, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###column variable names\n",
    "with open(file_dir+'/data/GIS/env_stacked/variable_list.txt') as f:\n",
    "      new_cols = f.readlines()\n",
    "\n",
    "var_names=[]\n",
    "for item in new_cols:\n",
    "    item=item.replace(\"\\n\",\"\")\n",
    "    var_names.append(item) \n",
    "len(var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "src=rasterio.open(file_dir+'/data/GIS/env_stacked/stacked_env_variables.tif')\n",
    "inRas=gdal.Open(file_dir+'/data/GIS/env_stacked/stacked_env_variables.tif')\n",
    "\n",
    "for i in taxa[\"taxon\"][:]:\n",
    "    data = pd.read_csv(file_dir+\"/data/spec_ppa/%s_ppa_dataframe.csv\"%i)\n",
    "    \n",
    "    spec = data[\"taxon_name\"][0]\n",
    "    spec = spec.replace(\" \",\"_\")\n",
    "    \n",
    "    print(\"processing species \", spec)\n",
    "    \n",
    "\n",
    "    #get all col and row values for species locations \n",
    "    len_pd=np.arange(len(data))\n",
    "    long=data[\"decimal_longitude\"]\n",
    "    lati=data[\"decimal_latitude\"]\n",
    "    ppa=data[\"present/pseudo_absent\"]\n",
    "\n",
    "    lon=long.values\n",
    "    lat=lati.values\n",
    "\n",
    "    row=[]\n",
    "    col=[]\n",
    "\n",
    "    #src=rasterio.open(raster)\n",
    "\n",
    "    for i in len_pd:\n",
    "        row_n, col_n = src.index(lon[i], lat[i])# spatial --> image coordinates\n",
    "        row.append(row_n)\n",
    "        col.append(col_n)\n",
    "\n",
    "    ##opening raster as 3d numpy array\n",
    "    #inRas=gdal.Open(raster)\n",
    "    myarray=inRas.ReadAsArray()\n",
    "    #print(myarray.shape)\n",
    "    #print(type(myarray))\n",
    "\n",
    "    #collect file with mean and std_dev for each band\n",
    "    mean_std=pd.read_csv(file_dir+'/data/GIS/env_bio_mean_std.txt',sep=\"\\t\")\n",
    "    mean_std=mean_std.to_numpy()\n",
    "\n",
    "\n",
    "    ########################################################\n",
    "    #extract the values for all bands and prepare input data\n",
    "    ########################################################\n",
    "    X=[]\n",
    "    species =[\"%s\"%spec]*int(len(row))\n",
    "\n",
    "    for j in range(0,186):\n",
    "        band=myarray[j]\n",
    "        x=[]\n",
    "\n",
    "        for i in range(0,len(row)):\n",
    "            value= band[row[i],col[i]]\n",
    "            if j < 46:\n",
    "                if value <-1000:\n",
    "                    value=np.nan\n",
    "                else:  \n",
    "                    value = ((value - mean_std.item((j,1))) / mean_std.item((j,2)))#scale values\n",
    "                x.append(value)\n",
    "                \n",
    "            if j >= 46:\n",
    "                if value <-1000:\n",
    "                    value=np.nan\n",
    "                else:  \n",
    "                    value=value\n",
    "                x.append(value)\n",
    "        X.append(x)\n",
    "    \n",
    "    \n",
    "\n",
    "    #set as numpy 2d array\n",
    "    X =np.array([np.array(xi) for xi in X])\n",
    "    #X\n",
    "    \n",
    "    #transform into dataframe and include row and column values\n",
    "    df=pd.DataFrame(X) \n",
    "    df=df.T\n",
    "    #df=\n",
    "    #df\n",
    "    \n",
    "    df[\"present/pseudo_absent\"]=ppa\n",
    "    df[\"decimal_latitude\"]=lati\n",
    "    df[\"decimal_longitude\"]=long\n",
    "    df[\"taxon_name\"]=species\n",
    "    df[\"present/pseudo_absent\"]=ppa\n",
    "    df[\"row_n\"]=row\n",
    "    df.rename(columns=dict(zip(df.columns[0:186], var_names)),inplace=True)\n",
    "    df=df.dropna(axis=0, how='any')\n",
    "    #df.head()\n",
    "    input_data=df\n",
    "    ##save input dataframe\n",
    "    input_data.to_csv(file_dir +\"/data/spec_ppa_env/%s_env_dataframe.csv\"%spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##opening raster as 3d numpy array\n",
    "inRas=gdal.Open(file_dir+'/data/GIS/env_stacked/stacked_env_variables.tif')\n",
    "myarray=inRas.ReadAsArray()\n",
    "print(myarray.shape)\n",
    "print(type(myarray))\n",
    "\n",
    "#get all col and row values for all cells on land \n",
    "df=pd.read_csv(file_dir+'/data/GIS/world_locations_to_predict.csv')\n",
    "len_pd=np.arange(len(df))\n",
    "print(len_pd)\n",
    "lon=df[\"decimal_longitude\"]\n",
    "lat=df[\"decimal_latitude\"]\n",
    "lon=lon.values\n",
    "lat=lat.values\n",
    "\n",
    "row=[]\n",
    "col=[]\n",
    "\n",
    "src=rasterio.open(file_dir+'/data/GIS/env_stacked/stacked_env_variables.tif')\n",
    "\n",
    "for i in len_pd:\n",
    "    row_n, col_n = src.index(lon[i], lat[i])# spatial --> image coordinates\n",
    "    row.append(row_n)\n",
    "    col.append(col_n)\n",
    "\n",
    "#collect file with mean and std_dev for each band\n",
    "mean_std=pd.read_csv(file_dir+'/data/GIS/env_bio_mean_std.txt',sep=\"\\t\")\n",
    "mean_std=mean_std.to_numpy()\n",
    "\n",
    "\n",
    "########################################################\n",
    "#extract the values for all bands and prepare input data\n",
    "########################################################\n",
    "X=[]\n",
    "\n",
    "for j in range(0,186):\n",
    "    print(j)\n",
    "    band=myarray[j]\n",
    "    x=[]\n",
    "\n",
    "    for i in range(0,len(row)):\n",
    "        if j < 46:\n",
    "            value= band[row[i],col[i]]\n",
    "            value = ((value - mean_std.item((j,1))) / mean_std.item((j,2)))#scale values\n",
    "            x.append(value)\n",
    "        if j >= 46:\n",
    "            value= band[row[i],col[i]]\n",
    "            x.append(value)\n",
    "    X.append(x)\n",
    "\n",
    "#X.append(x)\n",
    "#include row and column values\n",
    "X.append(row)\n",
    "X.append(col)\n",
    "#set as numpy 2d array\n",
    "X =np.array([np.array(xi) for xi in X])\n",
    "\n",
    "df=pd.DataFrame(X)\n",
    "\n",
    "df=df.T\n",
    "df.rename(columns=dict(zip(df.columns[0:186], var_names)),inplace=True)\n",
    "df=df.dropna(axis=0, how='any')\n",
    "df.head()\n",
    "\n",
    "input_X=df.iloc[:,0:186]\n",
    "np.shape(input_X)\n",
    "\n",
    "row=df[186]\n",
    "col=df[187]\n",
    "\n",
    "row_col=pd.DataFrame({\"row\":row,\"col\":col})\n",
    "\n",
    "#convert dataframe back to numpy array\n",
    "input_X=input_X.values\n",
    "#convert rows and col indices back to array\n",
    "row=row.values\n",
    "col=col.values\n",
    "\n",
    "#save\n",
    "prediction_array=np.save(file_dir+'/data/GIS/world_prediction_array.npy',input_X)\n",
    "prediction_pandas=row_col.to_csv(file_dir+'/data/GIS/world_prediction_row_col.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
