{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "We first set the notebook to display the output from each code block, <br>\n",
    "then import the required packages and set the file directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Mark.Rademaker\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36\\\\lib\\\\site-packages')\n",
    "\n",
    "import os\n",
    "\n",
    "import shap\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.ranking import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "try:\n",
    "    import keras\n",
    "    from imblearn.keras import balanced_batch_generator\n",
    "    from imblearn.under_sampling import NearMiss\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Activation\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.losses import categorical_crossentropy\n",
    "    from keras import regularizers\n",
    "\n",
    "except:\n",
    "    print(\"Keras not found\")\n",
    "    \n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def my_basename(path):\n",
    "    return os.path.splitext(os.path.split(path)[1])[0]\n",
    "\n",
    "\n",
    "\n",
    "file_dir=r'C:/Users/Mark.Rademaker/PycharmProjects/InternshipNaturalis/trait-geo-diverse-dl/data_extended'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text file is made in which to store the results of model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create text file to store results in and close again:\n",
    "with open(file_dir+'/results/_DNN_performance/DNN_eval.txt','w+') as file:\n",
    "    file.write(\"Species\"+\"\\t\"+\"Test_loss\"+\"\\t\"+\"Test_acc\"+\"\\t\"+\"Test_tpr\"+\"\\t\"+\"Test_AUC\"+\"\\t\"+\"Test_LCI95%\"+\"\\t\"+\"Test_UCI95%\"+\"\\t\"+\"occ_samples\"+\"\\t\"+\"abs_samples\"+\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The species names are loaded in to iterate through during training and testing. <br>\n",
    "Environmental variable names are stored in a list, to be used for determining feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access file with list of taxa names\n",
    "taxa=pd.read_csv(file_dir+\"/data/SQL_filtered_gbif/taxa_list.txt\",header=None)\n",
    "taxa.columns=[\"taxon\"] \n",
    "\n",
    "###column variable names\n",
    "with open(file_dir+'/data/GIS/env_stacked/variable_list.txt') as f:\n",
    "      new_cols = f.readlines()\n",
    "\n",
    "var_names=[]\n",
    "for item in new_cols:\n",
    "    item=item.replace(\"\\n\",\"\")\n",
    "    var_names.append(item) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training and testing\n",
    "For each species in the dataset, the DNN model is trained and tested. <br>\n",
    "This starts by opening the prepared dataset containing presences, pseudo-absences and associated environmental variable values. This dataframe then needs to be separated into a separate numpy array containing the environmental variable values (X) and presence/pseudo-absence (y). These arrays are further separated into a training (85% of samples) and test subset (15% of samples). Next the model is constructed and the training subset is fed to the model for training model weights. After training the test set is fed to the model and the output metric scores are saved to a text file. <br> The model weights and feature importance of the best performing model across five repeated runs are also saved to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species in taxa[\"taxon\"][:]:\n",
    "   \n",
    "    #open dataframe and rename columns\n",
    "    spec = species\n",
    "    table = pd.read_csv(file_dir +\"/data/spec_ppa_env/%s_env_dataframe.csv\"%spec)         \n",
    "    table.rename(columns=dict(zip(table.columns[1:42], var_names)),inplace=True)\n",
    "    \n",
    "    ####################################\n",
    "    #  filter dataframe for training   #\n",
    "    ####################################\n",
    "   \n",
    "    # drop any row with no-data values\n",
    "    table = table.dropna(axis=0, how=\"any\")\n",
    "\n",
    "\n",
    "    # make feature vector\n",
    "    band_columns = [column for column in table.columns[1:42]]\n",
    "    \n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for _, row in table.iterrows():\n",
    "        x = row[band_columns].values\n",
    "        x = x.tolist()\n",
    "        x.append(row[\"present/pseudo_absent\"])\n",
    "        X.append(x)\n",
    "\n",
    "    df = pd.DataFrame(data=X, columns=band_columns + [\"presence\"])\n",
    "    df.to_csv(\"filtered.csv\", index=None)\n",
    "\n",
    "    # extract n. of occ. and abs. samples\n",
    "    occ_len=int(len(df[df[\"presence\" ]==1]))\n",
    "    abs_len=int(len(df[df[\"presence\" ]==0]))\n",
    "    \n",
    "    ####################################\n",
    "    #  Numpy feature and target array  #\n",
    "    ####################################\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    band_columns = [column for column in df.columns[:-1]]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        X.append(row[band_columns].values.tolist())\n",
    "        y.append([1 - row[\"presence\"], row[\"presence\"]])\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    y = np.vstack(y)\n",
    "\n",
    "    ####################################\n",
    "    #    Split training and test set   #\n",
    "    ####################################\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y,random_state=42)\n",
    "    \n",
    "    test_set=pd.DataFrame(X_test)\n",
    "    test_set.rename(columns=dict(zip(test_set.columns[0:40], var_names)),inplace=True)\n",
    "    \n",
    "    shuffled_X_train=X_train.copy()\n",
    "    np.random.shuffle(shuffled_X_train)\n",
    "    shuffled_X_train=shuffled_X_train[:1000] # random subsample from test set for feature importance\n",
    "    \n",
    "    shuffled_X_test=X_test.copy()\n",
    "    np.random.shuffle(shuffled_X_test)\n",
    "    shuffled_X_test=shuffled_X_test[:1000] # random subsample from test set for feature importance\n",
    "    \n",
    "    ####################################\n",
    "    #      Training and testing        #\n",
    "    ####################################\n",
    "    \n",
    "    # prepare metrics\n",
    "    test_acc=[]\n",
    "    test_AUC=[]\n",
    "    test_tpr=[]\n",
    "    test_uci=[]\n",
    "    test_lci=[]\n",
    "\n",
    "   \n",
    "    Best_model_AUC=[0]\n",
    "    \n",
    "    # Five repetitions\n",
    "    for i in range(1,6):\n",
    "        print(\"run %s\"%i)\n",
    "        ###################\n",
    "        # Construct model #\n",
    "        ###################\n",
    "        batch_size = 75\n",
    "        num_classes = 2\n",
    "        epochs = 125\n",
    "\n",
    "        num_inputs = X.shape[1]  # number of features\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "        layer_1 = Dense(50, activation='relu',input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.000001))\n",
    "\n",
    "        layer_2 = Dense(25, activation='relu', input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.000001))\n",
    "        layer_3 = Dense(25, activation='relu', input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.0000001))\n",
    "        layer_4 = Dense(25, activation='relu', input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.00000001))\n",
    "\n",
    "\n",
    "        model.add(layer_1)\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(layer_2)\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(layer_3)\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(layer_4)\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "        out_layer = Dense(num_classes, activation=None)\n",
    "        model.add(out_layer)\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001), metrics =['accuracy'])\n",
    "        \n",
    "        ###############\n",
    "        # Train model #\n",
    "        ###############\n",
    "        training_generator,steps_per_epoch = balanced_batch_generator(X_train, y_train, sampler=NearMiss(), batch_size=75, random_state=42)\n",
    "        history = model.fit_generator(generator=training_generator, steps_per_epoch=steps_per_epoch, epochs=125, verbose=0)\n",
    "\n",
    "        ##############\n",
    "        # Test model #\n",
    "        ##############\n",
    "        score = model.evaluate(X_test, y_test, verbose=0)\n",
    "        predictions = model.predict(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:, 1], predictions[:, 1])\n",
    "        len_tpr=int(len(tpr)/2)\n",
    "      \n",
    "\n",
    "        #################\n",
    "        # Append scores #\n",
    "        #################\n",
    "        test_loss.append(score[0])\n",
    "        test_acc.append(score[1])\n",
    "        test_AUC.append(roc_auc_score(y_test[:, 1], predictions[:, 1]))\n",
    "        test_tpr.append(tpr[len_tpr])\n",
    "        AUC = roc_auc_score(y_test[:, 1], predictions[:, 1])\n",
    "\n",
    "        ###############################\n",
    "        # Create confidence intervals #\n",
    "        ###############################\n",
    "        n_bootstraps=1000\n",
    "        y_pred=predictions[:,1]\n",
    "        y_true=y_test[:,1]\n",
    "        rng_seed=42\n",
    "        bootstrapped_scores =[]\n",
    "\n",
    "\n",
    "        rng=np.random.RandomState(rng_seed)\n",
    "        for i in range (n_bootstraps):\n",
    "            #bootstrap by sampling with replacement on prediction indices\n",
    "            indices = rng.randint(0,len(y_pred)-1,len(y_pred))\n",
    "            if len (np.unique(y_true[indices])) <2:\n",
    "                continue\n",
    "\n",
    "            score = roc_auc_score(y_true[indices],y_pred[indices])\n",
    "            bootstrapped_scores.append(score)\n",
    "\n",
    "        sorted_scores=np.array(bootstrapped_scores)\n",
    "        sorted_scores.sort()\n",
    "\n",
    "        ci_lower=sorted_scores[int(0.05*len(sorted_scores))]\n",
    "        ci_upper=sorted_scores[int(0.95*len(sorted_scores))]\n",
    "     \n",
    "        test_lci.append(ci_lower)\n",
    "        test_uci.append(ci_upper)\n",
    "       \n",
    "    \n",
    "        ##############################################################\n",
    "        # Selection of best model across runs and feature importance #\n",
    "        ##############################################################\n",
    "    \n",
    "        #determine whether new model AUC is higher\n",
    "        if AUC > Best_model_AUC[0]:\n",
    "            # if yes save model to disk / overwrite previous model\n",
    "            Best_model_AUC[0]=AUC\n",
    "            model_json=model.to_json()\n",
    "            with open (file_dir+'/results/{}/{}_model.json'.format(spec,spec),'w') as json_file:\n",
    "                json_file.write(model_json)\n",
    "            model.save_weights(file_dir+'/results/{}/{}_model.h5'.format(spec,spec))\n",
    "            #if yes, save a figure of shap feature value impact    \n",
    "            \n",
    "            if int(len(X_train)) > 5000:           \n",
    "                explainer=shap.DeepExplainer(model,shuffled_X_train)\n",
    "                test_set=pd.DataFrame(shuffled_X_test)\n",
    "                test_set.rename(columns=dict(zip(test_set.columns[0:40], var_names)),inplace=True)\n",
    "                \n",
    "                shap_values=explainer.shap_values(shuffled_X_test)\n",
    "                fig=shap.summary_plot(shap_values[1],test_set,show=False)\n",
    "                plt.savefig(file_dir+'/results/{}/{}_feature_impact'.format(spec,spec),bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "            \n",
    "            else:\n",
    "                explainer=shap.DeepExplainer(model,X_train)\n",
    "                shap_values=explainer.shap_values(X_test)\n",
    "                fig=shap.summary_plot(shap_values[1],test_set,show=False)\n",
    "                plt.savefig(file_dir+'/results/{}/{}_feature_impact'.format(spec,spec),bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "            \n",
    "\n",
    "\n",
    "    # Model output metrics averaged across five runs to be written to file\n",
    "    avg_loss= sum(test_loss)/len(test_loss)\n",
    "    avg_acc = sum(test_acc)/len(test_acc)\n",
    "    avg_AUC = sum(test_AUC)/len(test_AUC)\n",
    "    avg_tpr = sum(test_tpr)/len(test_tpr)\n",
    "    avg_lci = sum(test_lci)/len(test_lci)\n",
    "    avg_uci = sum(test_uci)/len(test_uci)\n",
    "\n",
    "    # Write to file\n",
    "    with open(file_dir+'/results/_DNN_performance/DNN_eval.txt','a') as file:\n",
    "        file.write(spec+\"\\t\"+str(avg_loss)+\"\\t\"+str(avg_acc)+\"\\t\"+str(avg_tpr)+\"\\t\"+str(avg_AUC)+\"\\t\"+str(avg_lci)+\"\\t\"+str(avg_uci)+\"\\t\"+str(occ_len)+\"\\t\"+str(abs_len)+\"\\n\")       \n",
    "\n",
    "\n",
    "    #Next species!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after having processed all species, graphs with performance metric scores against the number of occurrence samples are created for a visual assessment of overall model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the file with model performance metrics\n",
    "data=pd.read_csv(file_dir+'/results/_DNN_performance/DNN_eval.txt',sep=\"\\t\")\n",
    "\n",
    "#set font size of axis\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "#separate the metrics\n",
    "x=data[\"occ_samples\"]\n",
    "x= [math.log10(i) for i in x]\n",
    "\n",
    "y1=data[\"Test_loss\"]\n",
    "y2=data[\"Test_acc\"]\n",
    "y3=data[\"Test_AUC\"]\n",
    "\n",
    "lci=data[\"Test_LCI95%\"]\n",
    "uci=data[\"Test_UCI95%\"]\n",
    "wci=data[\"Test_UCI95%\"]-data[\"Test_LCI95%\"]\n",
    "\n",
    "#Plot loss\n",
    "fig,ax = plt.subplots()\n",
    "plt.scatter(x,y1,c=\"red\")\n",
    "plt.xlabel(\"Occurrence samples\",fontweight='bold')\n",
    "plt.ylabel(\"Test loss\",fontweight='bold')\n",
    "labels=[item.get_text() for item in ax.get_xticklabels()]\n",
    "labels=['0','10','100','1000','10.000']\n",
    "ax.set_xticklabels(labels)\n",
    "fig.savefig(file_dir+'/results/_DNN_performance/Test_loss.png', dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Plot accuracy\n",
    "fig,ax = plt.subplots()\n",
    "plt.scatter(x,y2,c=\"orange\")\n",
    "plt.xlabel(\"Occurrence samples\",fontweight='bold')\n",
    "plt.ylabel(\"Test accuracy\",fontweight='bold')\n",
    "labels=[item.get_text() for item in ax.get_xticklabels()]\n",
    "labels=['0','10','100','1000','10.000']\n",
    "ax.set_xticklabels(labels)\n",
    "fig.savefig(file_dir+'/results/_DNN_performance/Test_acc.png', dpi=300,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "#Plot AUC\n",
    "fig,ax = plt.subplots()\n",
    "plt.scatter(x,y3)\n",
    "plt.xlabel(\"Occurrence samples\",fontweight='bold')\n",
    "plt.ylabel(\"Test AUC\",fontweight='bold')\n",
    "labels=[item.get_text() for item in ax.get_xticklabels()]\n",
    "labels=['0','10','100','1000','10.000']\n",
    "ax.set_xticklabels(labels)\n",
    "fig.savefig(file_dir+'/results/_DNN_performance/Test_AUC.png', dpi=300,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "#Plot confidence bands\n",
    "fig,ax = plt.subplots()\n",
    "points=plt.scatter(x,wci,c=y3,cmap=\"RdYlGn\")\n",
    "cbar=plt.colorbar(points)\n",
    "cbar.set_label(\"AUC value\")\n",
    "plt.xlabel(\"Occurrence samples\",fontweight='bold')\n",
    "plt.ylabel(\"Confidence band width\",fontweight='bold')\n",
    "labels=[item.get_text() for item in ax.get_xticklabels()]\n",
    "labels=['0','10','100','1000','10.000']\n",
    "ax.set_xticklabels(labels)\n",
    "fig.savefig(file_dir+'/results/_DNN_performance/Confidence_width.png', dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
