{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup \n",
    "We first set the notebook to display the output from each code block, <br>\n",
    "then import the required packages and set the file directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from shapely.affinity import scale\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "from pygbif.species import name_backbone\n",
    "import geopandas as gpd\n",
    "\n",
    "file_dir=r'C:/Users/Mark.Rademaker/PycharmProjects/InternshipNaturalis/trait-geo-diverse-dl/data_extended'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get backbone taxonomy\n",
    "A csv file was exported from the SQL database containing all occurrences for all ungulate species (834.182). A subset was made including only occurrences for which a full species name was available (797.372). As the same species might have been included multiple times under different names, a list was made containing all unique species names. The common taxonomic backbone species key and usage key for these labels were looked up from GBIF and put into a dataframe. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dataframes to be concatenated and filtered\n",
    "occ_all_species = pd.read_csv(file_dir+\"/data/SQL_raw_gbif/occurrences_all_species.csv\")\n",
    "df = occ_all_species[occ_all_species['label'].str.contains(\" \")]\n",
    "\n",
    "#Get unique label names\n",
    "unique_labels=df[\"label\"].unique()\n",
    "\n",
    "names = []\n",
    "back_key =[]\n",
    "remaining_labels=[]\n",
    "\n",
    "#Get backbone associated species names and taxon keys\n",
    "for item in unique_labels:\n",
    "    if \"species\" in name_backbone(item):\n",
    "        i = name_backbone(item)['species']\n",
    "        j = name_backbone(item)['usageKey']\n",
    "        names.append(i)\n",
    "        back_key.append(j)\n",
    "    else:\n",
    "        remaining_labels.append(item)\n",
    "        \n",
    "for item in remaining_labels:\n",
    "    value=name_backbone(item)['usageKey']\n",
    "    back_key.append(value)\n",
    "    names.append(item)\n",
    "    \n",
    "#Put into DataFrame\n",
    "df=pd.DataFrame({\"label\": unique_labels,\"back_key\": back_key,\"species\": names},columns=[\"label\",\"back_key\",\"species\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatening this dataframe with the dataset with occurrences for all species ensures a common species label for all instances belonging to the same species. Where no match can be found, the dataframe is filled with na values and these rows are dropped. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate with occurrence data, dataframe, drop na's \n",
    "df2=pd.merge(occ_all_species,df,how=\"left\",on=\"label\")\n",
    "\n",
    "df2 = df2[pd.notnull(df2['species'])]\n",
    "df2 = df2[pd.notnull(df2['decimal_latitude'])]\n",
    "df2 = df2[pd.notnull(df2['decimal_longitude'])]\n",
    "\n",
    "print(\"df2 without na's n.rows:\", len(df2.index))\n",
    "\n",
    "df2[\"back_key\"]=df2[\"back_key\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a separate dataframe is stored for each species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of species\n",
    "species = df2[\"species\"].unique()\n",
    "species.sort()\n",
    "\n",
    "#save separate dataframe for each species as csv file \n",
    "for spec in species:\n",
    "    data=df2.loc[df2['species'] == spec]\n",
    "    if len(data.index)>= 10:\n",
    "        spec=spec.replace(\" \",\"_\")\n",
    "        print(\"%s\"%spec, len(data.index))\n",
    "        data.to_csv(file_dir+'/data/SQL_raw_gbif/%s_raw_data.csv'%spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Filter occurrences based on criteria.\n",
    "The species dataframes with raw occurrences from GBIF are then filtered based on having >10 observations, possessing longitude and latitude data with >2 decimals, representing unique longitude-latitude locations, falling within the IUCN species range and having been collected >1900."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Open shapefile containing IUCN range of terrestrial mammals\n",
    "dist = file_dir+\"/data/IUCN_mammal_ranges/TERRESTRIAL_MAMMALS.shp\"\n",
    "dist_shp = gpd.read_file(dist)\n",
    "\n",
    "#create txt file with name of species included after filtering\n",
    "taxa_list=open(file_dir+'/data/SQL_filtered_gbif/taxa_list.txt',\"w\")\n",
    "\n",
    "\n",
    "#Filter occurrences per species\n",
    "for spec in species:\n",
    "    \n",
    "    data=df2.loc[df2['species'] == spec] #select subset of species\n",
    "    \n",
    "    # check >10 observations\n",
    "    if len(data.index)>= 10: \n",
    "\n",
    "        spec = spec.replace(\" \",\"_\")\n",
    "        print(\"processing species %s\"%spec)\n",
    "\n",
    "        data=pd.read_csv(file_dir+'/data/SQL_raw_gbif/%s_raw_data.csv'%spec) #load in data\n",
    "        \n",
    "        ###################################################\n",
    "        # check number of decimals longitude and latitude #\n",
    "        ###################################################\n",
    "        str_lat=(pd.Series.tolist(data[\"decimal_latitude\"].astype(str)))\n",
    "        str_lon=(pd.Series.tolist(data[\"decimal_longitude\"].astype(str)))\n",
    "        dec_lat=[]\n",
    "        dec_lon=[]\n",
    "\n",
    "        for i in range(len(str_lat)):\n",
    "    \n",
    "            if \"e\" in str_lat[i]:\n",
    "                str_lat[i]=\"0.00\"\n",
    "                decla = str_lat[i].split(\".\")[1]\n",
    "                print(i, str_lat[i],decla)\n",
    "                dec_lat.append(int(len(decla)))\n",
    "            else:\n",
    "                decla = str_lat[i].split(\".\")[1]\n",
    "                dec_lat.append(int(len(decla)))\n",
    "                \n",
    "        for i in range(len(str_lon)):\n",
    "            declo=str_lon[i].split(\".\")[1]\n",
    "            dec_lon.append(int(len(declo)))\n",
    "    \n",
    "        data[\"dec_lat\"]=dec_lat\n",
    "        data[\"dec_lon\"]=dec_lon\n",
    "\n",
    "        # filter only include those with min. 2 points\n",
    "        data=data[data[\"dec_lat\"] >= 2]\n",
    "        data=data[data[\"dec_lon\"] >= 2]\n",
    "        print(\"length only including lon-lat 2 decimals\",len(data.index))\n",
    "\n",
    "        data['coordinates'] = list(zip(data[\"decimal_longitude\"], data[\"decimal_latitude\"]))\n",
    "        data['lonlat'] = list(zip(data[\"decimal_longitude\"], data[\"decimal_latitude\"]))\n",
    "        data['coordinates'] = data[\"coordinates\"].apply(Point)\n",
    "\n",
    "        \n",
    "        #########################################\n",
    "        # only keep records with unique lon-lat #\n",
    "        #########################################\n",
    "        \n",
    "        data = data.drop_duplicates('lonlat')\n",
    "        print(\"length unique lon-lat\",len(data.index))\n",
    "\n",
    "       \n",
    "        ###############################################\n",
    "        # only keep records falling in IUNC range #\n",
    "        ###############################################\n",
    "        \n",
    "        speci=spec.replace(\"_\",\" \")\n",
    "        dist_shp_spec = dist_shp[dist_shp[\"binomial\"]== \"%s\"%speci]\n",
    "        poly_spec = dist_shp_spec[[\"geometry\"]]\n",
    "        \n",
    "        # merge the polygons\n",
    "        iucn_poly_spec= poly_spec.unary_union\n",
    "        Q3 = iucn_poly_spec.simplify(0.3)\n",
    "        Q3 #inspect polygon\n",
    "\n",
    "        if Q3.is_valid== False:\n",
    "            Q3 = Q3.buffer(0)\n",
    "\n",
    "        condition_list=[]\n",
    "\n",
    "        for point in data[\"coordinates\"]:\n",
    "            output= point.within(Q3)\n",
    "            condition_list.append(output)\n",
    "\n",
    "        #keep records that are in species range\n",
    "        data[\"in_dist_polygon\"]=condition_list\n",
    "        data2=data[data.in_dist_polygon == True]\n",
    "        print(\"length in species dist polygon\",len(data2.index))\n",
    "\n",
    "        #############################\n",
    "        # Only keep records > 1900  #\n",
    "        #############################\n",
    "        \n",
    "        data2['event_date'] = pd.to_datetime(data2['event_date']) # set date column to datetime format to extract year\n",
    "        data2['year'] = data2['event_date'].dt.year\n",
    "        data2['month']= data2['event_date'].dt.month\n",
    "\n",
    "        #set date column to datetime format and extract year\n",
    "        data2['event_date'] = pd.to_datetime(data2['event_date'])\n",
    "        data2['year'] = data2['event_date'].dt.year\n",
    "        data2['month']= data2['event_date'].dt.month\n",
    "\n",
    "        #only include observations >1900\n",
    "        data3=data2[data2.year >= 1900]\n",
    "        print(\"length observationas >1900\", len(data3.index))\n",
    "        \n",
    "        \n",
    "        # check >10 observations\n",
    "        if len(data3.index)>=10:\n",
    "            #save to csv\n",
    "            data3.to_csv(file_dir+'/data/SQL_filtered_gbif/%s_filtered_data.csv'%spec)\n",
    "            taxa_list.write(spec+\"\\n\")\n",
    "            \n",
    "#close text file\n",
    "taxa_list.close()\n",
    "# next species!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
