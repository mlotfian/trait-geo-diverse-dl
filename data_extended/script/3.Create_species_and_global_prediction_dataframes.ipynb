{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup \n",
    "We first set the notebook to display the output from each code block, <br>\n",
    "then import the required packages and set the file directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.plot import plotting_extent\n",
    "import gdal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas\n",
    "import rasterio\n",
    "import pycrs\n",
    "\n",
    "file_dir=r'C:/Users/Mark.Rademaker/PycharmProjects/InternshipNaturalis/trait-geo-diverse-dl/data_extended'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text file with the names of all the species we want to create the dataframes for is loaded in. <br>\n",
    "This can be used to call up the set of filtered occurrences for each species, and store this information in a dictionary.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access file with list of taxa names\n",
    "taxa=pd.read_csv(file_dir+\"/data/SQL_filtered_gbif/taxa_list.txt\",header=None)\n",
    "taxa.columns=[\"taxon\"]\n",
    "\n",
    "species_occ_dict={}\n",
    "\n",
    "for i in taxa[\"taxon\"]:\n",
    "    taxon_data = pd.read_csv(file_dir+\"/data/SQL_filtered_gbif/%s_filtered_data.csv\"%i)\n",
    "    #add species dataframe to dict\n",
    "    species_occ_dict[\"%s\"%i] = taxon_data  \n",
    "    #check whether all species have been included and inspect dictionary\n",
    "if len(species_occ_dict.keys())==len(taxa[\"taxon\"]):\n",
    "    print(\"All species dataframes now in dictionary\")\n",
    "else:\n",
    "    print(\"Error: not all species dataframe included\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raster clipping\n",
    "Next, we want to create a 1000km buffer around each occurrence point and merge these buffers into a single polygon. <br>\n",
    "The stacked raster map is clipped based on this polygon and saved, to have smaller files per species to use for pseudo-absence sampling in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in species_occ_dict:    \n",
    "    #load occurrence data and set initial projection\n",
    "    data=species_occ_dict[key]\n",
    "    spec = key\n",
    "\n",
    "\n",
    "    data['coordinates'] = list(zip(data[\"decimal_longitude\"], data[\"decimal_latitude\"]))\n",
    "    data['coordinates'] = data[\"coordinates\"].apply(Point)\n",
    "    data[\"present/pseudo_absent\"]=1\n",
    "    geo_data=geopandas.GeoDataFrame(data, geometry='coordinates',crs={'init' :'epsg:4326'})\n",
    "\n",
    "    #change projection to azimuthal equidistant to calculate 1000km buffer around point\n",
    "    geo_data = geo_data.to_crs({'init': 'esri:54032'}) \n",
    "    buffer=geo_data.buffer(1000*1000)\n",
    "    buffer=buffer.to_crs(epsg=4326)\n",
    "\n",
    "    #create single large polygon from individual buffers\n",
    "    union_buffer=buffer.unary_union\n",
    "\n",
    "    #first clip the raster based on this extend \n",
    "    raster=rasterio.open(file_dir+'/data/GIS/env_stacked/ENVIREM_BIOCLIM_stacked.tif')\n",
    "    \n",
    "    #specify output tif:\n",
    "    out_tif = file_dir+'/data/GIS/spec_stacked_raster_clip/%s_raster_clip.tif'%spec\n",
    "\n",
    "    #clip the raster:\n",
    "    out_img, out_transform = mask(dataset=raster, shapes=[union_buffer],crop=True)\n",
    "   \n",
    "    # Copy the metadata\n",
    "    out_meta = raster.meta.copy()\n",
    "\n",
    "    # Parse EPSG code\n",
    "    epsg_code = int(raster.crs.data['init'][5:])\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_img.shape[1],\n",
    "                     \"width\": out_img.shape[2],\n",
    "                     \"transform\": out_transform,\n",
    "                     \"crs\": pycrs.parse.from_epsg_code(epsg_code).to_proj4()})\n",
    "\n",
    "    with rasterio.open(out_tif, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect whether clip was correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the first band of the clipped raster for all species\n",
    "for key in species_occ_dict:\n",
    "    \n",
    "    # Extract occurrence point to plot on the raster (see if correct area was clipped)\n",
    "    data=species_occ_dict[key]\n",
    "    spec = key\n",
    "    data['coordinates'] = list(zip(data[\"decimal_longitude\"], data[\"decimal_latitude\"]))\n",
    "    data['coordinates'] = data[\"coordinates\"].apply(Point)\n",
    "    geo_data=geopandas.GeoDataFrame(data, geometry='coordinates',crs={'init' :'epsg:4326'})\n",
    "    \n",
    "    # open the clipped raster\n",
    "    clipped = rasterio.open(file_dir+'/data/GIS/spec_stacked_raster_clip/%s_raster_clip.tif'%spec)\n",
    "    array = clipped.read(1)\n",
    "    array_data = clipped.read(1,masked=True)\n",
    "    array_meta = clipped.profile\n",
    "   \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(array_data,cmap=\"gist_earth\",interpolation=\"none\",vmin=0,\n",
    "   \n",
    "    #Plot the occurrence points on the raster\n",
    "    extent=plotting_extent(clipped),)\n",
    "    spec_plots_points=geo_data[\"coordinates\"]\n",
    "    spec_plots_points.plot(ax=ax,\n",
    "                       marker='o',\n",
    "                       markersize=20,\n",
    "                       color='red')\n",
    "    ax.set_title(\"%s \\n Raster clip and occurrence points\"%spec,\n",
    "             fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "#Works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pseudo-absence sampling\n",
    "Now a selection of pseudo-absence points can be generated from within the raster clip.<br>\n",
    "We first open the raster and separate cells with data values (terrestrial) from cells with no-data values (sea). <br>\n",
    "Next we set cells that contain an occurrence location to a threshold value.\n",
    "\n",
    "A random selection of pseudo-absences is taken from the remaining cells with data values above the threshold. <br>\n",
    "The longitude and latitude values from the centre point of these cells are added to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in species_occ_dict:  \n",
    "    \n",
    "    #extract longitude and latitude of occurrence locations and label them as present (1)\n",
    "    presence_data = species_occ_dict[key]\n",
    "    presence_data[\"present/pseudo_absent\"]=1\n",
    "    spec = key\n",
    "    \n",
    "    long=presence_data[\"decimal_longitude\"]\n",
    "    lati=presence_data[\"decimal_latitude\"]\n",
    "    long=pd.Series.tolist(long)\n",
    "    lati=pd.Series.tolist(lati)\n",
    "   \n",
    "    #read raster\n",
    "    src=rasterio.open(file_dir+'/data/GIS/spec_stacked_raster_clip/%s_raster_clip.tif'%spec)\n",
    "    array=src.read_masks(1)\n",
    "    \n",
    "    # set raster cell mask values of presence locations to threshold value (=1) to exclude them from pseudo-absence sampling\n",
    "    for i in range(0,len(presence_data)):\n",
    "        row,col=src.index(long[i],lati[i])\n",
    "        array[row,col]=1\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(array,cmap=\"gray\")\n",
    "    ax.set_title(\"%s\"%spec,\n",
    "             fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    #subset of cells with datavalues from which to sample pseudo-absences\n",
    "    (y_index, x_index) = np.nonzero(array > 1)\n",
    "\n",
    "    #sample random locations from raster excluding sea and presence cells\n",
    "    r = gdal.Open(file_dir+'/data/GIS/spec_stacked_raster_clip/%s_raster_clip.tif'%spec)\n",
    "    (upper_left_x, x_size, x_rotation, upper_left_y, y_rotation, y_size) = r.GetGeoTransform()\n",
    "    \n",
    "    x_coords = x_index * x_size + upper_left_x + (x_size / 2) #add half the cell size\n",
    "    y_coords = y_index * y_size + upper_left_y + (y_size / 2) #to centre the point\n",
    "\n",
    "\n",
    "    lon_lat_array=np.stack((x_coords,y_coords)).T\n",
    "\n",
    "    #determine number of pseudo-absences to sample\n",
    "    random_sample_size=0\n",
    "    len_p=int(len(presence_data))\n",
    "    \n",
    "    if len_p >1000:\n",
    "        random_sample_size=len_p\n",
    "    else: \n",
    "        random_sample_size=1000\n",
    "   \n",
    "    random_sample_lon_lats=lon_lat_array[np.random.choice(lon_lat_array.shape[0], random_sample_size, replace=False), :] ##\n",
    "    print(len(random_sample_lon_lats), \"number of pseudo absences\")\n",
    "\n",
    "    #Add selected cells to dataset\n",
    "    lon=[]\n",
    "    lat=[]\n",
    "    psa=[0]*random_sample_size\n",
    "    taxon=[\"%s\"%spec]*random_sample_size\n",
    "    gbif=[\"no_id\"]*random_sample_size\n",
    "\n",
    "    for item in random_sample_lon_lats:\n",
    "        longitude=item[0]\n",
    "        latitude=item[1]\n",
    "        lon.append(longitude)\n",
    "        lat.append(latitude)\n",
    "\n",
    "    #Dataset including occurrences and pseudo-absence points\n",
    "    new_data=pd.DataFrame({\"gbif_id\": gbif,\"taxon_name\":taxon,\"decimal_longitude\": lon, \"decimal_latitude\":lat, \"present/pseudo_absent\": psa})\n",
    "    data=pd.concat([presence_data,new_data],ignore_index=True)\n",
    "    data=data[['taxon_name','gbif_id','decimal_longitude','decimal_latitude','present/pseudo_absent']]\n",
    "    data[\"taxon_name\"]=spec\n",
    "    data[\"row_n\"]=np.arange(len(data))\n",
    "     \n",
    "    long=data[\"decimal_longitude\"]\n",
    "    lati=data[\"decimal_latitude\"]\n",
    "    long=pd.Series.tolist(long)\n",
    "    lati=pd.Series.tolist(lati)\n",
    "    \n",
    "    print(len(data),\"lenght data with pseudo absences pre-filtering\")\n",
    "    \n",
    "    #read raster\n",
    "    src=rasterio.open(file_dir+'/data/GIS/spec_stacked_raster_clip/%s_raster_clip.tif'%spec)\n",
    "    array=src.read_masks(1)\n",
    "    \n",
    "    #remove potential presence locations in the sea due to potential erroneous sampling\n",
    "    for i in range(1,42):\n",
    "        array=src.read_masks(i)\n",
    "        for j in range(0,len(data)):\n",
    "            row,col=src.index(long[j],lati[j])\n",
    "            if array[row,col] ==0:\n",
    "                data=data[data.row_n != j]     \n",
    "    print(len(data), \"length data with pseudo absences post-filtering\")\n",
    "    \n",
    "    \n",
    "    data=data.reset_index(drop=True)\n",
    "    data.to_csv(file_dir + \"/data/spec_ppa/%s_ppa_dataframe.csv\"%spec)\n",
    "\n",
    "#next species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling environmental variable values \n",
    "Now we can use this dataset to extract the environmental variable values underneath each occurrence and pseudo-absence location. However, first the environmental variable values first need to be scaled to improve performance during training. \n",
    "This scaling is done based on the mean and standard deviation of each band (excluding no-data cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster=rasterio.open(file_dir+'/data/GIS/env_stacked/ENVIREM_BIOCLIM_stacked.tif')\n",
    "array = raster.read()\n",
    "profile=raster.profile\n",
    "\n",
    "with open(file_dir+'/data/GIS/env_bio_mean_std.txt','w+') as file:\n",
    "    file.write(\"band\"+\"\\t\"+\"mean\"+\"\\t\"+\"std_dev\"+\"\\n\")\n",
    "    file.close()\n",
    "\n",
    "#no data value might vary across band, try to find lowest no data value across raster bands\n",
    "min_abs=0\n",
    "\n",
    "for band in array:\n",
    "    minb=np.min(band)\n",
    "    if minb < min_abs:\n",
    "        min_abs=minb\n",
    "\n",
    "#create same minimum in each band\n",
    "for i in range(1,42):\n",
    "    print(i)\n",
    "    profile.update(count=1)\n",
    "    band=raster.read(i)\n",
    "    band[band == -9999] = min_abs\n",
    "    \n",
    "    #mask the cell values based on no data value\n",
    "    band_masked = np.ma.masked_array(band, mask=(band == min_abs)) \n",
    "    \n",
    "    #calculate mean and std.dev of each band\n",
    "    mean=band_masked.mean()\n",
    "    std_dev=np.std(band_masked)\n",
    "    \n",
    "    #write to file\n",
    "    with open(file_dir+'/data/GIS/env_bio_mean_std.txt','a') as file:\n",
    "        file.write(str(i)+\"\\t\"+str(mean)+\"\\t\"+str(std_dev)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting environmental predictor values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access file with list of taxa names\n",
    "taxa=pd.read_csv(file_dir+\"/data/SQL_filtered_gbif/taxa_list.txt\",header=None)\n",
    "taxa.columns=[\"taxon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in taxa[\"taxon\"]:\n",
    "    \n",
    "    data = pd.read_csv(file_dir+\"/data/spec_ppa/%s_ppa_dataframe.csv\"%i)\n",
    "    spec = data[\"taxon_name\"][0]\n",
    "    spec = spec.replace(\" \",\"_\") \n",
    "    print(\"processing species \", spec)\n",
    "    \n",
    "\n",
    "    #get all collumn and row numbers\n",
    "    len_pd=np.arange(len(data))\n",
    "    long=data[\"decimal_longitude\"]\n",
    "    lati=data[\"decimal_latitude\"]\n",
    "    ppa=data[\"present/pseudo_absent\"]\n",
    "\n",
    "    lon=long.values\n",
    "    lat=lati.values\n",
    "\n",
    "    row=[]\n",
    "    col=[]\n",
    "\n",
    "    src=rasterio.open(file_dir+'/data/GIS/spec_stacked_raster_clip/%s_raster_clip.tif'%spec)\n",
    "\n",
    "    for i in len_pd:\n",
    "        row_n, col_n = src.index(lon[i], lat[i])# spatial --> image coordinates\n",
    "        row.append(row_n)\n",
    "        col.append(col_n)\n",
    "\n",
    "    ##opening raster as 3d numpy array\n",
    "    inRas=gdal.Open(file_dir+'/data/GIS/spec_stacked_raster_clip/%s_raster_clip.tif'%spec)\n",
    "    myarray=inRas.ReadAsArray()\n",
    "    \n",
    "\n",
    "    #collect file with mean and std_dev for each band\n",
    "    mean_std=pd.read_csv(file_dir+'/data/GIS/env_bio_mean_std.txt',sep=\"\\t\")\n",
    "    mean_std=mean_std.to_numpy()\n",
    "\n",
    "\n",
    "    ##########################################################\n",
    "    #extract the values for all bands and prepare input data #\n",
    "    ##########################################################\n",
    "    X=[]\n",
    "    species =[\"%s\"%spec]*int(len(row))\n",
    "\n",
    "    for j in range(0,41):\n",
    "        band=myarray[j]\n",
    "        x=[]\n",
    "\n",
    "        for i in range(0,len(row)):\n",
    "            value= band[row[i],col[i]]\n",
    "            if value <-1000:\n",
    "                value=np.nan\n",
    "                x.append(value)\n",
    "            else:\n",
    "                value = ((value - mean_std.item((j,1))) / mean_std.item((j,2)))#scale values\n",
    "                x.append(value)\n",
    "        X.append(x)\n",
    "\n",
    "\n",
    "    #set as numpy 2d array\n",
    "    X =np.array([np.array(xi) for xi in X])\n",
    "\n",
    "    #transform into dataframe and include row and column values\n",
    "    df=pd.DataFrame(X)\n",
    "    df=df.T\n",
    "\n",
    "    df[\"present/pseudo_absent\"]=ppa\n",
    "    df[\"decimal_latitude\"]=lati\n",
    "    df[\"decimal_longitude\"]=long\n",
    "    df[\"taxon_name\"]=species\n",
    "    df[\"present/pseudo_absent\"]=ppa\n",
    "    df[\"row_n\"]=row\n",
    "    \n",
    "    #drop any potential rows with no-data values\n",
    "    df=df.dropna(axis=0, how='any')\n",
    "    input_data=df\n",
    "    \n",
    "    ##save input dataframe\n",
    "    input_data.to_csv(file_dir +\"/data/spec_ppa_env/%s_env_dataframe.csv\"%spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global prediction array\n",
    "Finally, we create a dataframe with the longitude and latitude values of all locations in world that the model later needs to be able to predict over. <br>\n",
    "In the second block we extract the environmental variable values at all these locations and store them in a numpy array that can be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open world raster map.\n",
    "r = gdal.Open(file_dir+'/data/GIS/env_stacked/ENVIREM_BIOCLIM_stacked.tif')\n",
    "\n",
    "# select subset of terrestrial cells with data values\n",
    "(y_index, x_index) = np.nonzero(array > 0) \n",
    "\n",
    "(upper_left_x, x_size, x_rotation, upper_left_y, y_rotation, y_size) = r.GetGeoTransform()\n",
    "x_coords = x_index * x_size + upper_left_x + (x_size / 2) #add half the cell size\n",
    "y_coords = y_index * y_size + upper_left_y + (y_size / 2) #to centre the point\n",
    "lon_lat_array=np.stack((x_coords,y_coords)).T\n",
    "\n",
    "#extract longitude and latitude for dataframe\n",
    "lon=[]\n",
    "lat=[]\n",
    "\n",
    "for item in lon_lat_array:\n",
    "    longitude=item[0]\n",
    "    latitude=item[1]\n",
    "    lon.append(longitude)\n",
    "    lat.append(latitude)\n",
    "\n",
    "data_to_pred=pd.DataFrame({\"decimal_longitude\":lon,\"decimal_latitude\":lat})\n",
    "data_to_pred.to_csv(file_dir + \"/data/GIS/world_locations_to_predict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##opening raster as 3d numpy array\n",
    "inRas=gdal.Open(file_dir+'/data/GIS/env_stacked/ENVIREM_BIOCLIM_stacked.tif')\n",
    "myarray=inRas.ReadAsArray()\n",
    "\n",
    "\n",
    "#get all collumn and row values for all cells to predict over\n",
    "df=pd.read_csv(file_dir+'/data/GIS/world_locations_to_predict.csv')\n",
    "\n",
    "len_pd=np.arange(len(df))\n",
    "lon=df[\"decimal_longitude\"]\n",
    "lat=df[\"decimal_latitude\"]\n",
    "lon=lon.values\n",
    "lat=lat.values\n",
    "\n",
    "row=[]\n",
    "col=[]\n",
    "\n",
    "src=rasterio.open(file_dir+'/data/GIS/env_stacked/ENVIREM_BIOCLIM_stacked.tif')\n",
    "\n",
    "for i in len_pd:\n",
    "    row_n, col_n = src.index(lon[i], lat[i])# spatial --> image coordinates\n",
    "    row.append(row_n)\n",
    "    col.append(col_n)\n",
    "\n",
    "#collect file with mean and std_dev for each band\n",
    "mean_std=pd.read_csv(file_dir+'/data/GIS/env_bio_mean_std.txt',sep=\"\\t\")\n",
    "mean_std=mean_std.to_numpy()\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# extract the values for all bands and prepare input data #\n",
    "###########################################################\n",
    "X=[]\n",
    "\n",
    "for j in range(0,41):\n",
    "    print(j)\n",
    "    band=myarray[j]\n",
    "    x=[]\n",
    "\n",
    "    for i in range(0,len(row)):\n",
    "        value= band[row[i],col[i]]\n",
    "        if value <-1000:\n",
    "            value=np.nan\n",
    "            x.append(value)\n",
    "        else:\n",
    "            value = ((value - mean_std.item((j,1))) / mean_std.item((j,2))) # scale values\n",
    "            x.append(value)\n",
    "    X.append(x)\n",
    "\n",
    "#include row and column values\n",
    "X.append(row)\n",
    "X.append(col)\n",
    "\n",
    "#set as numpy 2d array\n",
    "X =np.array([np.array(xi) for xi in X])\n",
    "\n",
    "df=pd.DataFrame(X)\n",
    "df=df.T\n",
    "\n",
    "#drop any rows with no-data values\n",
    "df=df.dropna(axis=0, how='any')\n",
    "input_X=df.loc[:,0:40]\n",
    "\n",
    "row=df[41]\n",
    "col=df[42]\n",
    "\n",
    "row_col=pd.DataFrame({\"row\":row,\"col\":col})\n",
    "\n",
    "#convert dataframe back to numpy array\n",
    "input_X=input_X.values\n",
    "#convert rows and col indices back to array\n",
    "row=row.values\n",
    "col=col.values\n",
    "\n",
    "#save\n",
    "prediction_array=np.save(file_dir+'/data/GIS/world_prediction_array.npy',input_X)\n",
    "prediction_pandas=row_col.to_csv(file_dir+'/data/GIS/world_prediction_row_col.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
