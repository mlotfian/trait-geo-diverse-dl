{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Mark.Rademaker\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36\\\\lib\\\\site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from deepviz.guided_backprop import GuidedBackprop\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics.ranking import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from deepviz.guided_backprop import GuidedBackprop\n",
    "\n",
    "try:\n",
    "    import keras\n",
    "    from imblearn.keras import balanced_batch_generator\n",
    "    from imblearn.under_sampling import NearMiss\n",
    "    import keras.backend as K\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Activation\n",
    "    from keras.optimizers import RMSprop\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.optimizers import Adagrad\n",
    "    from keras.optimizers import SGD\n",
    "    from keras.callbacks import LambdaCallback, ReduceLROnPlateau, ModelCheckpoint\n",
    "    from keras.layers.core import Lambda\n",
    "    from keras.losses import categorical_crossentropy\n",
    "    from keras import regularizers\n",
    "\n",
    "except:\n",
    "    print(\"Keras not found\")\n",
    "    \n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def my_basename(path):\n",
    "    return os.path.splitext(os.path.split(path)[1])[0]\n",
    "\n",
    "file_dir=r'C:/Users/Mark.Rademaker/PycharmProjects/InternshipNaturalis/trait-geo-diverse-dl/data_extended'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access file with list of taxa names\n",
    "taxa=pd.read_csv(file_dir+\"/data/SQL_filtered_gbif/taxa_list.txt\",header=None)\n",
    "taxa.columns=[\"taxon\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create text file to store results in and close again:\n",
    "with open(file_dir+'/results/_DNN_performance/DNN_eval.txt','w+') as file:\n",
    "    file.write(\"Species\"+\"\\t\"+\"Test_loss\"+\"\\t\"+\"Test_acc\"+\"\\t\"+\"Test_tpr\"+\"\\t\"+\"Test_AUC\"+\"\\t\"+\"Test_LCI95%\"+\"\\t\"+\"Test_UCI95%\"+\"\\t\"+\"occ_samples\"+\"\\t\"+\"abs_samples\"+\"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###column variable names\n",
    "with open(file_dir+'/data/GIS/env_stacked/variable_list.txt') as f:\n",
    "      new_cols = f.readlines()\n",
    "\n",
    "var_names=[]\n",
    "for item in new_cols:\n",
    "    item=item.replace(\"\\n\",\"\")\n",
    "    var_names.append(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species in taxa[\"taxon\"][95:]:\n",
    "   # try:\n",
    "    print(species)\n",
    "    spec = species\n",
    "\n",
    "    #prepare dataframe for training, include weight for presence obserevations and readd col names\n",
    "    table = pd.read_csv(file_dir +\"/data/spec_ppa_env/%s_env_dataframe.csv\"%spec)         \n",
    "    table.rename(columns=dict(zip(table.columns[1:42], var_names)),inplace=True)\n",
    "    #table.head()\n",
    "\n",
    "\n",
    "    # print(len(table.index))\n",
    "    table = table.dropna(axis=0, how=\"any\")\n",
    "\n",
    "\n",
    "    # make feature vector\n",
    "    band_columns = [column for column in table.columns[1:42]]\n",
    "    #print(band_columns)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for _, row in table.iterrows():\n",
    "        x = row[band_columns].values\n",
    "        x = x.tolist()\n",
    "        x.append(row[\"present/pseudo_absent\"])\n",
    "        X.append(x)\n",
    "\n",
    "    df = pd.DataFrame(data=X, columns=band_columns + [\"presence\"])\n",
    "    #df.head()\n",
    "    df.to_csv(\"filtered.csv\", index=None)\n",
    "\n",
    "    ###########################################\n",
    "    occ_len=int(len(df[df[\"presence\" ]==1]))\n",
    "    #print(occ_len)\n",
    "    abs_len=int(len(df[df[\"presence\" ]==0]))\n",
    "    #print(abs_len)\n",
    "    ############################################\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    band_columns = [column for column in df.columns[:-1]]\n",
    "    # print(band_columns)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        X.append(row[band_columns].values.tolist())\n",
    "        y.append([1 - row[\"presence\"], row[\"presence\"]])\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    #print(X)\n",
    "    #print(type(X))\n",
    "    print(X.shape)\n",
    "    y = np.vstack(y)\n",
    "\n",
    "\n",
    "    ######################### The actual model#####################\n",
    "    #########################                 #####################\n",
    "    #########################                 #####################\n",
    "    test_loss=[]\n",
    "    test_acc=[]\n",
    "    test_AUC=[]\n",
    "    test_tpr=[]\n",
    "    test_uci=[]\n",
    "    test_lci=[]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y,random_state=42)\n",
    "    \n",
    "    ####################\n",
    "    test_set=pd.DataFrame(X_test)\n",
    "    test_set.rename(columns=dict(zip(test_set.columns[0:40], var_names)),inplace=True)\n",
    "    \n",
    "    shuffled_X_train=X_train.copy()\n",
    "    np.random.shuffle(shuffled_X_train)\n",
    "    shuffled_X_train=shuffled_X_train[:1000]\n",
    "    \n",
    "    shuffled_X_test=X_test.copy()\n",
    "    np.random.shuffle(shuffled_X_test)\n",
    "    shuffled_X_test=shuffled_X_test[:1000]\n",
    "    \n",
    "    ####################\n",
    "    Best_model_AUC=[0]\n",
    "    ####################\n",
    "    for i in range(1,6):\n",
    "        print(\"run %s\"%i)\n",
    "\n",
    "        batch_size = 75\n",
    "        num_classes = 2\n",
    "        epochs = 125\n",
    "\n",
    "        num_inputs = X.shape[1]  # number of features\n",
    "\n",
    "\n",
    "        model = Sequential()\n",
    "        layer_1 = Dense(50, activation='relu',input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.000001))\n",
    "\n",
    "        layer_2 = Dense(25, activation='relu', input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.000001))\n",
    "        layer_3 = Dense(25, activation='relu', input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.0000001))\n",
    "        layer_4 = Dense(25, activation='relu', input_shape=(num_inputs,))#, kernel_regularizer=regularizers.l1(0.00000001))\n",
    "\n",
    "\n",
    "        model.add(layer_1)\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(layer_2)\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(layer_3)\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(layer_4)\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "        out_layer = Dense(num_classes, activation=None)\n",
    "        model.add(out_layer)\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        model.compile(loss=\"categorical_crossentropy\",\n",
    "                    # optimizer =SGD(lr=0.001, momentum =0.9, nesterov=True),\n",
    "                    # optimizer=Adagrad(lr=0.001),\n",
    "                    # optimizer=RMSprop(lr=0.001),# rho=0.9, epsilon=1e-08, decay=0.0),\n",
    "                    optimizer=Adam(lr=0.001),#, rho=0.9, epsilon=1e-08, decay=0.0),\n",
    "                    metrics =['accuracy'])\n",
    "        training_generator,steps_per_epoch = balanced_batch_generator(X_train, y_train, sampler=NearMiss(), batch_size=75, random_state=42)\n",
    "        history = model.fit_generator(generator=training_generator, steps_per_epoch=steps_per_epoch, epochs=125, verbose=0)\n",
    "\n",
    "        score = model.evaluate(X_test, y_test, verbose=0)\n",
    "        #print('Test loss:', score[0])\n",
    "        #print('Test accuracy:', score[1])\n",
    "        predictions = model.predict(X_test)\n",
    "       # print(\"AUC\", roc_auc_score(y_test[:, 1], predictions[:, 1]))\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:, 1], predictions[:, 1])\n",
    "        len_tpr=int(len(tpr)/2)\n",
    "       # print(len_tpr)\n",
    "       # print(\"true positive rate\", tpr[len_tpr])\n",
    "        #plt.plot(fpr, tpr)\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "        test_loss.append(score[0])\n",
    "        test_acc.append(score[1])\n",
    "        test_AUC.append(roc_auc_score(y_test[:, 1], predictions[:, 1]))\n",
    "        test_tpr.append(tpr[len_tpr])\n",
    "        AUC = roc_auc_score(y_test[:, 1], predictions[:, 1])\n",
    "\n",
    "        ###confidence intervals#######################\n",
    "        ##############################################\n",
    "        n_bootstraps=1000\n",
    "        y_pred=predictions[:,1]\n",
    "        y_true=y_test[:,1]\n",
    "        rng_seed=42\n",
    "        bootstrapped_scores =[]\n",
    "\n",
    "\n",
    "        rng=np.random.RandomState(rng_seed)\n",
    "        for i in range (n_bootstraps):\n",
    "            #bootstrap by sampling with replacement on prediction indices\n",
    "            indices = rng.randint(0,len(y_pred)-1,len(y_pred))\n",
    "            if len (np.unique(y_true[indices])) <2:\n",
    "                continue\n",
    "\n",
    "            score = roc_auc_score(y_true[indices],y_pred[indices])\n",
    "            bootstrapped_scores.append(score)\n",
    "\n",
    "        sorted_scores=np.array(bootstrapped_scores)\n",
    "        sorted_scores.sort()\n",
    "\n",
    "        ci_lower=sorted_scores[int(0.05*len(sorted_scores))]\n",
    "        ci_upper=sorted_scores[int(0.95*len(sorted_scores))]\n",
    "       # print(\"CI for the score: [{:0.3f}-{:0.3f}]\".format(ci_lower,ci_upper))\n",
    "\n",
    "        test_lci.append(ci_lower)\n",
    "        test_uci.append(ci_upper)\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "\n",
    "\n",
    "        #determine whether new model AUC is higher\n",
    "        if AUC > Best_model_AUC[0]:\n",
    "            # if yes save model to disk / overwrite previous model\n",
    "            Best_model_AUC[0]=AUC\n",
    "            model_json=model.to_json()\n",
    "            with open (file_dir+'/results/{}/{}_model.json'.format(spec,spec),'w') as json_file:\n",
    "                json_file.write(model_json)\n",
    "            model.save_weights(file_dir+'/results/{}/{}_model.h5'.format(spec,spec))\n",
    "            #if yes, save a figure of shap feature value impact    \n",
    "            \n",
    "            if int(len(X_train)) > 5000:           \n",
    "                explainer=shap.DeepExplainer(model,shuffled_X_train)\n",
    "                test_set=pd.DataFrame(shuffled_X_test)\n",
    "                test_set.rename(columns=dict(zip(test_set.columns[0:40], var_names)),inplace=True)\n",
    "                \n",
    "                shap_values=explainer.shap_values(shuffled_X_test)\n",
    "                fig=shap.summary_plot(shap_values[1],test_set,show=False)\n",
    "                plt.savefig(file_dir+'/results/{}/{}_feature_impact'.format(spec,spec),bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "            \n",
    "            else:\n",
    "                explainer=shap.DeepExplainer(model,X_train)\n",
    "                shap_values=explainer.shap_values(X_test)\n",
    "                fig=shap.summary_plot(shap_values[1],test_set,show=False)\n",
    "                plt.savefig(file_dir+'/results/{}/{}_feature_impact'.format(spec,spec),bbox_inches=\"tight\")\n",
    "                plt.close()\n",
    "            #print('saved model to disk')\n",
    "\n",
    "\n",
    "    ####################\n",
    "    avg_loss= sum(test_loss)/len(test_loss)\n",
    "    avg_acc = sum(test_acc)/len(test_acc)\n",
    "    avg_AUC = sum(test_AUC)/len(test_AUC)\n",
    "    avg_tpr = sum(test_tpr)/len(test_tpr)\n",
    "    avg_lci = sum(test_lci)/len(test_lci)\n",
    "    avg_uci = sum(test_uci)/len(test_uci)\n",
    "\n",
    "\n",
    "    with open(file_dir+'/results/_DNN_performance/DNN_eval.txt','a') as file:\n",
    "        file.write(spec+\"\\t\"+str(avg_loss)+\"\\t\"+str(avg_acc)+\"\\t\"+str(avg_tpr)+\"\\t\"+str(avg_AUC)+\"\\t\"+str(avg_lci)+\"\\t\"+str(avg_uci)+\"\\t\"+str(occ_len)+\"\\t\"+str(abs_len)+\"\\n\")       \n",
    "\n",
    "\n",
    "\n",
    "  #  except:\n",
    "   #       pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create graphs for evaluating occurrence sample size on dnn performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "data=pd.read_csv(file_dir+'/results/_DNN_performance/DNN_eval.txt',sep=\"\\t\")\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "#data\n",
    "x=data[\"occ_samples\"]\n",
    "x= [math.log10(i) for i in x]\n",
    "\n",
    "y1=data[\"Test_loss\"]\n",
    "y2=data[\"Test_acc\"]\n",
    "#y3=data[\"Test_tpr\"]\n",
    "y4=data[\"Test_AUC\"]\n",
    "#n=data[\"Test_AUC\"].round(3)\n",
    "lci=data[\"Test_LCI95%\"]\n",
    "uci=data[\"Test_UCI95%\"]\n",
    "wci=data[\"Test_UCI95%\"]-data[\"Test_LCI95%\"]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "plt.scatter(x,y1,c=\"red\")\n",
    "plt.xlabel(\"Occurrence samples\",fontweight='bold')\n",
    "plt.ylabel(\"Test loss\",fontweight='bold')\n",
    "labels=[item.get_text() for item in ax.get_xticklabels()]\n",
    "labels=['0','10','100','1000','10.000']\n",
    "ax.set_xticklabels(labels)\n",
    "fig.savefig(file_dir+'/results/_DNN_performance/Test_loss.png', dpi=300,bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "plt.scatter(x,y2,c=\"orange\")\n",
    "plt.xlabel(\"Occurrence samples\",fontweight='bold')\n",
    "plt.ylabel(\"Test accuracy\",fontweight='bold')\n",
    "labels=[item.get_text() for item in ax.get_xticklabels()]\n",
    "labels=['0','10','100','1000','10.000']\n",
    "ax.set_xticklabels(labels)\n",
    "fig.savefig(file_dir+'/results/_DNN_performance/Test_acc.png', dpi=300,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "#fig = plt.figure()\n",
    "#plt.scatter(x,y3,c=\"green\")\n",
    "#plt.xlabel(\"occurrence samples\")\n",
    "#plt.ylabel(\"Test_tpr\")\n",
    "#fig.savefig(file_dir+'/results/Test_acc.png', dpi=fig.dpi)\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "plt.scatter(x,y4)\n",
    "plt.xlabel(\"Occurrence samples\",fontweight='bold')\n",
    "plt.ylabel(\"Test AUC\",fontweight='bold')\n",
    "labels=[item.get_text() for item in ax.get_xticklabels()]\n",
    "labels=['0','10','100','1000','10.000']\n",
    "ax.set_xticklabels(labels)\n",
    "fig.savefig(file_dir+'/results/_DNN_performance/Test_AUC.png', dpi=300,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "points=plt.scatter(x,wci,c=y4,cmap=\"RdYlGn\")\n",
    "cbar=plt.colorbar(points)\n",
    "cbar.set_label(\"AUC value\")\n",
    "plt.xlabel(\"Occurrence samples\",fontweight='bold')\n",
    "plt.ylabel(\"Confidence band width\",fontweight='bold')\n",
    "labels=[item.get_text() for item in ax.get_xticklabels()]\n",
    "labels=['0','10','100','1000','10.000']\n",
    "ax.set_xticklabels(labels)\n",
    "fig.savefig(file_dir+'/results/_DNN_performance/Confidence_width.png', dpi=300,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "#for i, txt in enumerate(n):\n",
    " #   plt.annotate(txt, (x[i], y[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
